{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crowd Counting Pipeline — Training on Colab\n",
        "\n",
        "This notebook clones the repository and runs the training scripts on a GPU.\n",
        "All logic lives in the Python package — this notebook just orchestrates.\n",
        "\n",
        "**Requirements**: Colab with GPU runtime (Runtime > Change runtime type > GPU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Clone Repo and Install\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "!git clone https://github.com/YOUR_USERNAME/crowd-counting-pipeline.git\n",
        "%cd crowd-counting-pipeline\n",
        "\n",
        "# Install the package with training dependencies\n",
        "%pip install -e \".[train]\" -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Download from Kaggle (requires kaggle.json)\n",
        "# Upload your kaggle.json first, then run:\n",
        "# !mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "# !python -m crowd_counting.data.download --output-dir ./data\n",
        "\n",
        "# Option B: If you already have the dataset on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Adjust path to where your dataset is on Drive\n",
        "!ln -sf /content/drive/MyDrive/ShanghaiTech ./data/ShanghaiTech\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Verify GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train on ShanghaiTech Part B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with wandb logging (sign in when prompted)\n",
        "!python scripts/train.py --config configs/shb.yaml --wandb --epochs 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train on ShanghaiTech Part A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on Part A (denser crowds, adaptive sigma)\n",
        "!python scripts/train.py --config configs/sha.yaml --wandb --epochs 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on Part B test set\n",
        "!python scripts/evaluate.py --config configs/shb.yaml --checkpoint weights/best_model.pth --save-viz\n",
        "\n",
        "# Evaluate on Part A test set\n",
        "!python scripts/evaluate.py --config configs/sha.yaml --checkpoint weights/best_model.pth --save-viz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Export to ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/export_onnx.py --checkpoint weights/best_model.pth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Weights to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "drive_output = '/content/drive/MyDrive/crowd-counting-weights'\n",
        "os.makedirs(drive_output, exist_ok=True)\n",
        "\n",
        "# Copy trained weights\n",
        "for f in ['best_model.pth', 'checkpoint.pth', 'csrnet.onnx']:\n",
        "    src = f'weights/{f}'\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, drive_output)\n",
        "        print(f'Saved {f} to Drive')\n",
        "\n",
        "# Copy evaluation visualizations\n",
        "if os.path.exists('outputs/evaluation_samples.png'):\n",
        "    shutil.copy2('outputs/evaluation_samples.png', drive_output)\n",
        "    print('Saved evaluation visualization to Drive')\n",
        "\n",
        "print(f'\\nAll outputs saved to: {drive_output}')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
